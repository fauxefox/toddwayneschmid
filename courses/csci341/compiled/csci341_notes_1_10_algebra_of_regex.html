<!-- THIS IS A COMPILED FILE 
 Compiled on 2025-10-07 -->



































































































<!DOCTYPE html>

<head>
	
	<meta charset="UTF-8">
	<meta name="author" content="Todd Schmid">
	
	<!-- Renders latex formulas -->
	<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
	<!-- Renders tikz figures mid page -->
    <link rel="stylesheet" type="text/css" href="https://tikzjax.com/v1/fonts.css">
    <script src="https://tikzjax.com/v1/tikzjax.js"></script>
	
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	
	<link rel="stylesheet" href="../../../styles.css" />
	<link rel="stylesheet" href="../../course_styles.css" />
	<link rel="icon" href="../../../squidab.png" />
	
	<!-- * Customize for each course * -->
	<title>CSCI 341 Theory of Computation</title>
	<meta name="description" content="Course pages for CSCI 341.">
	<meta name="keywords" content="bucknell,theory,computer,science,schmid,csci341">
	<!-- ***************************** -->
</head>

<body>

<div id="content">

<!-- Header Section -->
<div id="course_head" style="text-align: left;">
	<h1 style="margin-bottom: 0; text-align: left;">CSCI 341 Theory of Computation</h1>
	<i>Fall 2025, with <a href="../../../teaching.html">Schmid</a></i>
	<div id="links">
		<!-- * Customize for each course * -->
		<a href="csci341_index.html"><span class="link">Syllabus</span></a>
		<a href="csci341_contents.html"><span class="link">Notes</span></a>
		<a href="csci341_resources.html"><span class="link">Resources</span></a>
		<a href= "csci341_assignments.html"><span class="link">Assignments</span></a>
		<!-- ***************************** -->
	</div>
</div>



<div id="stuff">


<!-- BEGINNING OF BODY -->
<div style="border: none; margin-bottom: -25px; padding: 0px; text-align: right;"><a href="../compiled/csci341_notes_1_09_antimirov_derivatives.html"><span class="link"> &larr; antimirov derivatives</span></a><a href="../compiled/csci341_notes_1_11_kleenes_theorem.html"><span class="link">kleenes theorem &rarr;</span></a></div><h1>The Algebra of Regular Languages</h1>

<p>
    So far, we have seen that every regular language is finitely presentable, formally \(\mathsf{Reg} \subseteq \mathsf{Fin}\).
    Continuing our journey towards the proof of Kleene's Theorem, which states that \(\mathsf{Reg} = \mathsf{Fin}\), we need to gain a bit more proficiency with regular expressions.
    The most important step in the proof of the reverse containment is to show that certain <i>systems of equations</i> involving regular expressions can be <i>solved</i>.
    This process of reasoning with equations between regular expressions is called the <i>algebra of regular expressions</i>, and can be really fun once you get used to it. 
    If solving systems of equations doesn't sound like <i>algebra</i> to you, then I'm not sure what will.
</p>

<!-- <h2>Regular Algebra</h2> -->

<p>
    Let's get a bit more formal about what all this is about.
</p>

<div class="definition">
    <b>(Language Equivalence)</b>
    Given regular expressions \(r_1, r_2\in \mathit{RExp}\), we say that \(r_1\) and \(r_2\) are <i>language equivalent</i> if \(\mathcal L(r_1) = \mathcal L(r_2)\), and in such a case we will write \(r_1 =_{\mathcal L} r_2\).
</div>

<p>
    It may come as a surprise to you that different regular expressions can be language equivalent.
    It helps to think of arithmetic: \(5 + 2 = 2 + 5\), even though those two <i>arithmetic expressions</i> are different. 
    One might call these two arithmetic expressions <i>number equivalent</i>.
    In a similar fashion, the regular expressions \(a + b\) and \(b + a\) (over an alphabet containing \(a\) and \(b\)) are language equivalent. 
    Indeed, 
    \[
        \mathcal L(a + b) = \{a, b\} = \{b, a\} = \mathcal L(b + a)
    \]
    Not too crazy, right?
</p>

<div class="exercise">
    <b>(Basic Equations)</b>
    Calculate the language semantics for each of the regular expressions below.
    Which regular expressions are language equivalent?
    <ol>
        <li>\((ab + b) + c\)</li>
        <li>\(c + (a + \varepsilon) b\)</li>
        <li>\(a(b + c)\)</li>
        <li>\((b + c)a\)</li>
    </ol>
</div>

<h3>Unions</h3>
<p>
    Here is another example of language equivalent regular expressions:
    \[
        a + (b + c) =_{\mathcal L} (a + b) + c
    \]
    Again, remember that regular expressions are just sequences of symbols, so the two regular expressions that appear above are not exactly the same.
    However, it's not hard to prove that they are <i>language equivalent</i>: on the one hand, we have
    \[\begin{aligned}
        \mathcal L(a + (b + c))
        &= \mathcal L(a) \cup \mathcal L(b + c) \\
        &= \mathcal L(a) \cup (\mathcal L(b) \cup \mathcal L(c)) \\
        &= \{a\} \cup (\{b\} \cup \{c\}) \\
        &= \{a\} \cup \{b,c\} \\
        &= \{a,b,d\} 
    \end{aligned}\]
    On the other, we also have 
    \[
        \mathcal L((a + b) + c) = \{a, b, c\}   \hspace{5em} (\star)
    \]
    Therefore, \(\mathcal L((a + b) + c) = \mathcal L(a + (b + c))\), and we can write \((a + b) + c =_{\mathcal L} a + (b + c)\).
</p>    

<div class="exercise">
    <b>(Calculating a Union)</b>
    Recreate the calculation of \(\mathcal L(a + (b + c))\) above, but for \(\mathcal L((a + b) + c)\) to produce \((\star)\).
    What is the difference between calculating the values of \(\mathcal L(a + (b + c))\) and \(\mathcal L((a + b) + c)\)?
</div>

<div class="lemma">
    <b>(Everything About Unions)</b>
    Let \(r,r_1,r_2,r_3 \in \mathit{RExp}\) be any regular expressions. 
    Then
    <ol>
        <li>\(r_1 + r_2 =_{\mathcal L} r_2 + r_1\)</li>
        <li>\(r + \emptyset =_{\mathcal L} r\)</li>
        <li>\(\emptyset + r =_{\mathcal L} r\)</li>
        <li>\(r + r =_{\mathcal L} r\)</li>
        <li>\(r_1 + (r_2 + r_3) =_{\mathcal L} (r_1 + r_2) + r_3\)</li>
    </ol>
</div>

<div class="exercise">
    <b>(Understanding the Union Rules)</b>
    Use the equations in Everything About Unions to derive the equation 
    \[
        (ab + b) + (ba + \emptyset)
        =_{\mathcal L} ba + (b + ab)
    \]
    Label each equation you write down with the number of the equation you used in the Everything About Unions Lemma.
    For example, as a first couple steps (I'm not saying these are the correct first steps), you might write 
    \[\begin{aligned}
        (ab + b) + (ba + \emptyset)
        &= (ab + b) + (\emptyset + ba) \hspace{5em} (\text{Unions 1.}) \\
        &= ((ab + b) + \emptyset) + ba \hspace{5em} (\text{Unions 5.}) \\
    \end{aligned}\]
</div>

<h3>Sequential Composition</h3>
<p>
    Here is another example of a pair of language equivalent regular expressions: \(a(bc) =_{\mathcal L} (ab)c\).
    Here, 
    \[\begin{aligned}
        \mathcal L(a (bc)) 
        &= \mathcal L(a) \cdot \mathcal L(bc) \\
        &= \mathcal L(a) \cdot (\mathcal L(b) \cdot \mathcal L(c)) \\
        &= \{a\} \cdot (\{b\} \cdot \{c\}) \\
        &= \{a\} \cdot (\{bc\}) \\
        &= \{abc\} \\
    \end{aligned}\]
    This is what we should expect; the language semantics of a word, represented as a regular expression, should be the set containing only that word.
    But there is also the other way of forming the word "\(abc\)", namely \((ab)c\), which also has the language semantics
    \[\mathcal L((a b)c) = \{abc\}\]
    Therefore, \(a(bc) =_{\mathcal L} (ab)c\). 
</p>

<p>
    There are a few more equations to do with sequential composition that are going to be useful later.
</p>

<div class="lemma">
    <b>(Everything About Sequential Composition)</b>
    Let \(r,r_1,r_2,r_3 \in \mathit{RExp}\) be any regular expressions. 
    We have
    <ol>
        <li>\(r \cdot \varepsilon =_{\mathcal L} r\) and \(\varepsilon \cdot r =_{\mathcal L} r\)</li>
        <li>\(r\cdot \emptyset =_{\mathcal L} \emptyset\) and \(\emptyset \cdot r =_{\mathcal L} \emptyset\)</li>
        <li>\(r_1 \cdot (r_2 \cdot r_3) =_{\mathcal L} (r_1 \cdot r_2) \cdot r_3\)</li>
        <li>\(r_1 \cdot (r_2 + r_3) =_{\mathcal L} (r_1 \cdot r_2) + (r_1 \cdot r_3)\)</li>
        <li>\((r_1 + r_2) \cdot r_3 =_{\mathcal L} (r_1 \cdot r_3) + (r_2 \cdot r_3)\)</li>
    </ol>
</div>

<div class="exercise">
    <b>(Do Expression Commute?)</b>
    In artithmetic, we have the lovely equation \(n \times m = m \times n\) for all \(n,m \in \mathbb N\).
    For which regular expressions \(r_1,r_2 \in \mathit{RExp}\) is the equation \(r_1 \cdot r_2 =_{\mathcal L} r_2 \cdot r_1\) true?
    Assume an alphabet of \(A = \{a,b\}\).
</div>

<div class="problem">
    <b>(Distributing on the Left)</b>
    Let \(r_1,r_2,r_3 \in \mathit{RExp}\).
    Prove the equation 
    \[r_1 \cdot (r_2 + r_3) =_{\mathcal L} (r_1 \cdot r_2) + (r_1 \cdot r_3)\]
    by calculating the language on either side of the equation and arguing that these two languages are equal.
    <div class="hint">
        Let \(L_i = \mathcal L(r_i)\) for \(i=1,2,3\).
        An element of \(L_1\cdot(L_2 \cup L_3)\) is a word of the form \(wu\) where \(w \in L_1\) and either \(u \in L_2\) or \(u \in L_3\). 
        What does an element of \((L_1 \cdot L_2) \cup (L_1 \cdot L_3)\) look like?
    </div>
</div>

<h3>Kleene Star</h3>
<p>
    So far, we have dealt with unions and sequential composition. 
    The last operation on our list to deal with is the Kleene star, which is... let's just say, a lot less familiar.
    The gist of the equations we are about to see is this:
    <i>
        The Kleene star of a language consists of the empty word (if it is not already there), as well as any concatenation of the words in the language (including repetitions).
    </i>
</p>

<p>
    For example, for a letter \(a \in A\), unraveling the definition of the language semantics of \(a^*\) gives 
    \[\mathcal L(a^*) = \{a^n \mid n \in \mathbb N\}\] 
    For \(n = 0\), \(a^n = a^0 = \varepsilon\).
    For \(n > 0\), \(a^n = a a^{n-1}\). 
    Unravelling the equation above, we can write down the following calculation:
    \[\begin{aligned}
        \mathcal L(a^*) 
        &= \{a^0\} \cup \{a^n \mid n \in \mathbb N \text{ and } n > 0\}  \\
        &= \{a^0\} \cup \{aa^{n-1} \mid n \in \mathbb N \text{ and } n > 0\}  \\
        &= \{\varepsilon\} \cup \{aa^n \mid n \in \mathbb N\} \\
        &= \{\varepsilon\} \cup (\{a\} \cdot \{a^n \mid n \in \mathbb N\})  \\
        &= \mathcal L(\varepsilon) \cup (\mathcal L(a) \cdot \mathcal L(a^*))  \\
        &= \mathcal L(\varepsilon + (a \cdot a^*))  \\
    \end{aligned}\]
    In other words, 
    \(
        a^* =_{\mathcal L} \varepsilon + aa^*
    \).
</p>

<div class="exercise">
    <b>(Superfluous Addition)</b>
    Let \(a \in A\).
    Calculate the language semantics of \((\varepsilon + a)^*\).
    Is there a simpler regular expression that this one is equivalent to?
</div>

<p>
    The exercise and example above point to the following two equations.
</p>

<div class="lemma">
    <b>(Basics About the Kleene Star)</b>
    Let \(r \in \mathit{RExp}\). 
    Then 
    <ol>
        <li>\(\varepsilon + rr^* =_{\mathcal L} r^*\)</li>
        <li>\(\varepsilon + r^*r =_{\mathcal L} r^*\)</li>
        <li>\((\varepsilon + r)^* =_{\mathcal L} r^*\)</li>
    </ol>
</div>

<div class="exercise">
    <b>(Using the Basic Equations)</b>
    Let \(a,b \in A\).
    Prove the following identities.
    <ol>
        <li>\((\varepsilon + a)^* =_{\mathcal L} (\emptyset + a)^*\)</li>
        <li>\(\varepsilon + a^* =_{\mathcal L} a^*\)</li>
        <li>\(\emptyset^* =_{\mathcal L} \varepsilon\)</li>
        <li>\((a + b)^* =_{\mathcal L} a(a + b + \varepsilon)^* + b(\emptyset + b + a)^* + \varepsilon\)</li>
        <li>for any \(r_i,s_i \in \mathit{RExp}\), \((r_1 + r_2)(s_1 + s_2) =_{\mathcal L} r_1s_1 + r_1s_2 + r_2s_1 + r_2s_2\)</li>
    </ol>
</div>

<h3>Arden's Rule</h3>

<p>
    There is one equation we are missing from our toolset so far. 
    It's not quite an <i>equation</i> so much as it is a rule, since it only applies in certain situations. 
    The situations it applies to have to do with the <i>empty word property</i>.
</p>

<div class="definition">
    <b>(Empty Word Property)</b>
    Let \(r \in \mathit{RExp}\).
    Then \(r\) has the <i>empty word property</i> if \(\varepsilon \in \mathcal L(r)\).
</div>

<p>
    So, for example, the regular expression \(\varepsilon\) does have the empty word property, while \(\emptyset\) does not.
    And for any \(a \in A\), we have \(\mathcal L(a) = \{a\}\), so \(a\) does not have the empty word property. 
    On the other hand, for every regular expression \(r \in \mathit{RExp}\), \(r^*\) does have the empty word property.
</p>

<div class="theorem">
    <b>(Arden's Rules)</b>
    Let \(r,s,t \in \mathit{RExp}\) be regular expressions. 
    Assume that \(r\) does not have the empty word property.
    Then 
    <ul>
        <li>(Left Rule) if \(s =_{\mathcal L} t + r\cdot s\), then \(s =_{\mathcal L} r^* \cdot t\)</li>
        <li>(Right Rule) if \(s =_{\mathcal L} t + s\cdot r\), then \(s =_{\mathcal L} t \cdot r^*\)</li>
    </ul>
</div>

<p>
    Typically, Arden's Left Rule is just called "Arden's Rule". 
    Arden's Left Rule is all that's needed for the next lecture, but the Right rule is going to make things a lot easier for you in the exercises!
</p>

<p>
    Remember the equation \(r^* =_{\mathcal L} \varepsilon + rr^*\) from the Basics of the Kleene Star. 
    This equation reveals to us that \(r^*\) <i>solves</i> the following equation for an unknown variable \(x\),
    \[
        x =_{\mathcal L} \varepsilon + r \cdot x
    \]
    Arden's rule tells us more: it says that if \(r\) does not have the empty word property, then \(r^*\) is the <i>only</i> solution to the equation above.
</p>

<div class="example">
    <b>(Deleting Pluses)</b>
    Let \(a,b \in A\).
    Here is a pretty whacky equation you might not expect:
    \[
        (a + b)^* =_{\mathcal L} (a^*b)^*a^*
    \]
    But it's actually not a difficult application of Arden's rule.
    Let \(s = (a + b)^*\).
    Then we have 
    \[\begin{aligned}
        s   &= (a + b)^*                                  && \text{(definition of \(s\))}\\
            &= \varepsilon + (a + b)(a + b)^*             && \text{(Kleene Star 1)}\\
            &= \varepsilon + a(a + b)^* + b(a + b)^*      && \text{(Seq Comp 4)}\\
            &= \varepsilon + as + bs                      && \text{(definition of \(s\))}\\
            &= \varepsilon + bs + as                      && \text{(Union 1)}\\
            &= a^*(\varepsilon + bs)                      && \text{(Arden's Rule with \(r = a\) and \(t = \varepsilon + bs\))}\\
            &= a^*\varepsilon + a^*bs                     && \text{(Seq Comp 4)}\\
            &= a^* + a^*bs                                && \text{(Seq Comp 1)}\\
            &= (a^*b)^*a^*                                && \text{(Arden's Rule with \(r = a^*b\) and \(t = a^*\))}
    \end{aligned}\]
    Above, Arden's Rule is applied twice: once with \(r = a\) and once with \(r = a^*b\) (\(s\) is \((a + b)^*\) throughout). 
    Neither \(r\) has the empty word property: it can be seen directly that \(\mathcal L(a)\) does not contain \(\varepsilon\), and by definition every word in \(\mathcal L(a^*b)\) must end with a \(b\), so \(a^*b\) doesn't have the empty word property either.
</div>

<div class="exercise">
    <b>(Using Arden's Rules)</b>
    Use the equations above to prove the following equations.
    <ol>
        <li>\(aa^* = a^* a\)</li>
        <li>\(a^* =_{\mathcal L} (aa)^*(a + \varepsilon)\).</li>
    </ol>
</div>

<div class="exercise">
    <b>(Breaking Arden's Rule)</b>
    What does the empty word property have to do with Arden's Rule at all?
    Find two solutions \(r_1,r_2 \in \mathit{RExp}\) to the equation 
    \[
        x =_{\mathcal L} \emptyset + \varepsilon \cdot x
    \]
    such that \(r_1 \neq_{\mathcal L} r_2\).
    What went wrong?
</div>

<div class="challenge-exercise">
    <b>(aa...Aaa..<a href="https://youtu.be/O04H-z1z1T0?si=CBtEBf88xJ5xnm-c" target="_blank">.</a>.AAAA)</b>
    Let \(a \in A\).
    Use the equations in the Lemmas above to prove that \[a^* =_{\mathcal L} (a^*)^*\]
    Label each equation you use in your proof to indicate which lemma was used where. 
</div>

<div class="problem">
    <b>(Air Flare)</b>
    Let \(a,b \in A\).
    Use the equations in the Lemmas above to prove the following equations:
    <ol>
        <li>\(a^*a^* =_{\mathcal L} a^*\)</li>
        <li>\((a + b)^* =_{\mathcal L} b^*(ab^*)^*\)</li>
    </ol>
    Label each equation you use in your proof to indicate which lemma was used where. 
    <div class="hint">
        Using Arden's Rules 1. is very helpful in the first one!
    </div>
</div>


<h2>Proof of Arden's Rule</h2>
<p>
    We are now ready to prove Arden's Rule, but it is worth mentioning that this is an important place where <i>strong induction</i> comes up. 
</p>

<p>
    Remember that <i>induction</i> states that if a subset \(S \subseteq \mathbb N\) is upwards-closed and contains \(0\), then \(S = \mathbb{N}\). 
    There are actually two ways of establishing that such a set \(S\) is upwards-closed and contains \(0\):
</p>
<ol>
        <li>
            The first way is what you are used to: showing that \(0 \in S\) and also that if \(n \in S\), then \(n + 1 \in S\) as well. This way is ordinary induction.
        </li>
        <li>
            The second way is maybe more convoluted-feeling, but equivalent: the second way is to show that for any \(n \in \mathbb N\), if \(m \in S\) for all \(m < n\), then \(n \in S\). 
            To see why this works, first note that there are no natural numbers \(m < 0\), so vaccuously \(0 \in S\). 
            Then for any \(m < 1\), \(m \in S\), because only \(0 < 1\).
            Likewise, if \(0, 1, 2, \dots, n \in S\), then \(n + 1 \in S\) as well.
            This shows that \(S\) is upwards-closed, so by induction, \(S = \mathbb N\).
        </li>
    </ol>

<div class="proof">
    <b>(of Arden's Rule)</b>
    Let \(r,s,t \in \mathit{RExp}\) and let \(L = \mathcal L(s)\), \(U = \mathcal L(r)\), and \(V = \mathcal L(t)\).
    Assume that \(r\) does not have the empty word property, and that \(s =_{\mathcal L} t + rs\). 
    In other words, we are assuming that \(\varepsilon \notin U\) and \(L = V \cup U\cdot L\).
    We need to show that \(L = U^* \cdot V\).
    We are going to show that (1) \(L \subseteq U^* \cdot V\) and (2) that \(L \supseteq U^* \cdot V\).

    <p>
        Let's start by proving (1).
        Let \(w \in L\). 
        We need to prove that \(w \in U^*\cdot V\).
        We are going to do this by <i>strong induction on the length of \(w\)</i>. 
    </p>
    <p>
        A proof by strong induction starts with the induction hypothesis: <i>suppose that for any word \(u \in L\) with \(|u| < |w|\), we have \(u \in U^* \cdot V\).</i>
        We now need to show that it follows from this supposition that \(w \in U^* \cdot V\).
        Since \(L = V \cup U\cdot L\), \(w\) is of one of two forms: either \(w \in V\) or \(w \in U\cdot L\).
        This means there are two cases to consider.
        <ul>
            <li>
                In the first case, \(w \in V\), and we need to show that \(w \in U^*\cdot V\).
                This follows from the definition of the Kleene star for a language: \(\varepsilon \in U\), so \(\varepsilon w \in U^* \cdot V\). 
                Since \(\varepsilon w = w\), \(w \in U^*\cdot V\) and we are done with this case.
            </li>
            <li>
                In the second case, \(w \in U \cdot L\).
                In this case, \(w\) is of the form \(w = uv\) for some \(u \in U\) and \(v \in L\). 
                Again, we need to argue that \(w \in U^*\cdot V\).
                But \(u \in U\) implies that \(u \neq \varepsilon\), because \(r\) does not have the empty word property and \(U = \mathcal L(r)\).
                This means that \(|u| > 0\), so \(|v| < |u| + |v| = |uv| = |w|\).
                By the induction hypothesis, \(v \in L\) and \(|v| < |w|\) means that \(v \in U^* \cdot V\).
                Therefore, there is a \(u' \in U^*\) and a \(v' \in V\) such that \(v = u'v'\). 
                Then \(w = uu'v'\). 
                Since \(u,u' \in U^*\), their concatenation \(uu' \in U^*\) as well. 
                It follows that \(w = uu'v \in U^*V\), and we are done with this case.
            </li>
        </ul>
    </p>

    <p>
        Now we show (2), that \(L \supseteq U^* \cdot V\). 
        Let \(w \in U^* \cdot V\). 
        Then \(w = uv\) for some \(u\in U^*\) and \(v \in V\). 
        We are going to show that \(uv \in L\) by strong induction on \(u\).
        This means that our induction hypothesis states that <i>if \(u'v \in L\) for all \(u' \in U\) with \(|u'| < |u|\), then \(uv \in L\)</i>.
    </p>
    <p>
        If \(u = \varepsilon\), then \(uv = v \in V \cup U\cdot L = L\), so \(w = uv \in L\). 
        Otherwise, \(u = u_1u_2\) for some \(u_1,u_2 \in U\) with \(|u_1| > 0\). 
        This means that \(u_2v \in U^*V\) and \(|u_2| < |u|\), so the induction hypothesis tells us that \(u_2v \in L\). 
        But concatenating with \(u_1\), we get 
        \[
            w = uv = u_1u_2v \in U \cdot L \subseteq V \cup U \cdot L = L
        \]
        so that \(w \in L\).
        This shows that \(L \supseteq U^* \cdot V\).
    </p>
    <p>
        Since (1) \(L \subseteq U^* \cdot V\) and (2) \(L \supseteq U^* \cdot V\), \(L = U^* \cdot V\). 
        Therefore, \(r =_{\mathcal L} s^* \cdot t\).
    </p>
</div>


<!-- OPTIONAL STUFF -->
<!-- <h2>(Optional Material) Salomaa's Completeness Theorem</h2>

<p>
    It turns out that any time we want to show that two regular expressions are language equivalent, actually computing languages is totally unnecessary: all we need are a handful of equations and rules.
    Here are a few of the more obvious rules:
</p>

<div class="theorem">
    <b>(Equational Reasoning)</b>
    For any regular expressions \(r,r_1,r_2,r_3 \in \mathit{RExp}\), 
    \[\begin{aligned}
        \text{(Reflexive)} &&& r =_{\mathcal L} r \\
        \text{(Symmetric)} &&& \text{if \(r_1 =_{\mathcal L} r_2\), then \(r_2 =_{\mathcal L} r_1\)} \\
        \text{(Transitive)} &&& \text{if \(r_1 =_{\mathcal L} r_2\) and \(r_2 = r_3\), then \(r_1 =_{\mathcal L} r_3\)} \\
    \end{aligned}\]
    Also, for any regular expressions \(r,s,r_1,r_2,s_1,s_2 \in \mathit{RExp}\), 
    \[\begin{aligned}
        \text{(Congruence-\(+\))} &&& \text{
            if \(r_1 =_{\mathcal L} s_1\) and \(r_2 =_{\mathcal L} s_2\), then \(r_1 + r_2 =_{\mathcal L} s_1 + s_2\)
        } \\
        \text{(Congruence-\(\cdot\))} &&& \text{
            if \(r_1 =_{\mathcal L} s_1\) and \(r_2 =_{\mathcal L} s_2\), then \(r_1 \cdot r_2 =_{\mathcal L} s_1 \cdot s_2\)
        } \\
        \text{(Congruence-\(*\))} &&& \text{
            if \(r =_{\mathcal L} s\), then \(r^* =_{\mathcal L} s^*\)
        } \\
    \end{aligned}\]
</div>

<p>
    The Reflexive, Symmetric, and Transitive rules are kind of innate when we think of equality. 
    The Congruence rules might be a little less intuitive, but a few examples might help:
    we have laready seen that \((a + b) + c =_{\mathcal L} a + (b + c)\) and \(a(bc) =_{\mathcal L} (ab)c\). 
    The Congruence rules tell us that 
    \[
        ((a + b) + c) + a(bc) =_{\mathcal L} (a + (b + c)) + (ab)c
    \]
    That's pretty nice, right? 
    We didn't have to compute any languages at all to determine these two were language equivalent. 
</p>

<p>
    There are a ton more equations we can use, but here are the basic ones:
</p>

<div class="lemma">

</div> --><div style="border: none; margin-bottom: -25px; padding: 0px; text-align: right;"><a href="../compiled/csci341_notes_1_09_antimirov_derivatives.html"><span class="link"> &larr; antimirov derivatives</span></a><a href="../compiled/csci341_notes_1_11_kleenes_theorem.html"><span class="link">kleenes theorem &rarr;</span></a></div>


<!-- END OF BODY -->
    <div style="height: 50px;border-top: 1px solid rgb(131, 131, 131);margin-top: 50px; padding: 20px; text-align: right;">
        <a href=""><span class="link">Top</span></a>
    </div>
</div>
</div>
</body>


</html>