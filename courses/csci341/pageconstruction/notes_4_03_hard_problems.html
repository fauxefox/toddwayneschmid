<h1>Hard Problems</h1>

<p>
    Finding a resource-intensive solution so a problem is not evidence that the problem is difficult. 
    For example, the problem of sorting an array can be solved in various dumb ways: <a href="https://en.wikipedia.org/wiki/Bogosort" target="_blank">bogosort</a>, at one end, runs in \(\mathcal O(n!)\)-time, and merge sort at the other runs in \(\mathcal O(n\log(n))\)-time.
    Sorting ain't that bad!
</p>

<p>
    Big-O analysis of worst-case runtime is a sort of upper-bound on the difficulty of an algorithmic problem.
    Once we find an algorithm for solving a problem in \(\mathcal O(f)\)-time, we know that it is "at most \(\mathcal O(f)\)-difficult".
    This opens up the possibility of beating that time, thereby showing that the problem is not as hard as we thought.
</p>

<p>
    On the other hand, Big-\(\Omega\) analysis gives a lower bound on the (typically worst-case) running time of an algorithm. 
    But with Turing machines, complexity is a bit fuzzier: translating between a Turing machine and a physical computer is not always linear, and the Church-Turing thesis in particular does not directly make any observation about the complexity of translating between different versions of the deterministic Turing machine (like multi-tape and multiple-to-one-symbols; both of which take \(\mathcal O(n^2)\)-time).
    This fuzziness was our motivation to consider the families \(\mathsf{P}\), \(\mathsf{NP}\), \(\mathsf{EXP}\), and so on.
    In this lecture, our goal is to give a useful lower-bound on the complexity of an algorithm in this fuzzy paradigm of complexity classes.
</p>

<!--  -->
<h2>Hardness</h2>

<p>
    The intuitive notion of "one problem is harder than another", as many of you have probably discovered by now, has to do with reductions. 
    What we learn from a reduction \(L_1 \preceq L_2\) (i.e., there exists a reduction \(r \colon L_1 \preceq L_2\)) is that \(L_2\) was the "harder" problem: any solution to \(L_2\) is enough to solve the "easier" problem \(L_1\).
    This relation \(\preceq \subseteq 2^{A^*}\) is a kind of "relative hardness" measure for problems. 
    This is where the following terminology comes from.
</p>

<div class="definition">
    <b>(\(\mathsf{Fam}\)-Hardness)</b>
    Let \(\mathsf{Fam} \subseteq 2^{A^*}\) be any family of languages, and let \(L \subseteq A^*\) be a language. 
    We say that \(L\) is <i>\(\mathsf{Fam}\)-hard</i> if for any \(U \in \mathsf{Fam}\), there is a polynomial time reduction \(r \colon U \preceq L\).
</div>

<p>
    So, for example, a decision problem is <i>\(\mathsf{P}\)-hard</i> if it has a polynomial time reduction from every tractible problem.
    The broader class of \(\mathsf{NP}\)-hard problems get a special name because they are the prime focus of the \(\mathsf{P}=\mathsf{NP}\) problem mentioned last time.
</p>

<div class="definition">
    <b>(Hard Problems)</b>
    A decision problem (i.e., language) \(L \subseteq A^*\) is generally called <i>hard</i> if \(L\) is \(\mathsf{NP}\)-hard.
</div>

<p>
    A lot of your favourite problems are hard, unfortunately.
    Many puzzle and videogames have been shown to be \(\mathsf{NP}\)-hard, including 
    <ul>
        <li><a href="https://minesweepergame.com/math/minesweeper-is-np-complete-2018.pdf" target="_blank">Minesweeper</a></li>
        <li><a href="https://www.cs.ox.ac.uk/people/paul.goldberg/FCS/sudoku.html" target="_blank">Sudoku</a></li>
        <li><a href="https://ualberta.scholaris.ca/server/api/core/bitstreams/9031e952-751a-405c-bf3d-25bcf9b4f5d9/content" target="_blank">Sokoban</a></li>
        <li><a href="https://arxiv.org/pdf/2003.09914" target="_blank">Rush Hour</a></li>
        <li><a href="https://journals.sagepub.com/doi/10.3233/ICG-2004-27303" target="_blank">Battleship</a></li>
        <li><a href="https://theses.liacs.nl/pdf/2012-01JanvanRijn_2.pdf" target="_blank">Mahjong</a></li>
        <li>...(and many others)</li>
    </ul>
    What this means is that any algorithm for deciding whether a particular configuration of the puzzle (of arbitrary size---these results often generalize the game so that the game board can be arbitrarily large) has a solution <i>at all</i> can be used to solve any other computational problem in \(\mathsf{NP}\) with only a polynomial amount of added runtime.
</p>

<p>
    Directly showing that a decision problem is \(\mathsf{NP}\)-hard is so difficult that some of the most fundamental results of the field are proofs of this form.
    What is more typical is to show that a problem is \(\mathsf{NP}\)-hard by exhibiting a polynomial time reduction to another problem that is \(\mathsf{NP}\)-hard.
</p>

<div class="theorem">
    <b>(Hardness by Reduction)</b>
    Let \(\mathsf{Fam} \subseteq 2^{A^*}\) be a family of languages, and let \(L \subseteq A^*\) be \(\mathsf{Fam}\)-hard. 
    Suppose that we had another language \(W \subseteq A^*\) and a polynomial time reduction \(r \colon L \preceq W\).
    Show that \(W\) is also \(\mathsf{Fam}\)-hard.
</div>

<div class="problem">
    <b>(Proving Hardness by Reduction)</b>
    Prove the Hardness by Reduction theorem.
</div>

<p>
    We are going to see a reduction in action a little bit later.
</p>

<!--  -->
<h2>Completeness</h2>

<p>
    A slightly more confusing property of a problem with respect to a complexity class is <i>completeness</i>.
</p>

<div class="definition">
    <b>(\(\mathsf{Fam}\)-completeness)</b>
    Let \(\mathsf{Fam} \subseteq 2^{A^*}\) be a family of languages. 
    A language \(L \subseteq A^*\) is said to be <i>\(\mathsf{Fam}\)-complete</i> if \(L \in \mathsf{Fam}\) and \(L\) is \(\mathsf{Fam}\)-hard.
</div>

<p>
    <a href="https://www.cs.bme.hu/~friedl/alg/games.pdf" target="_blank">Many puzzle games, including some of those mentioned at the top</a>, are in fact \(\mathsf{NP}\)-complete, in addition to be \(\mathsf{NP}\)-hard.
    A long list of \(\mathsf{NP}\)-complete problems <a href="https://en.wikipedia.org/wiki/List_of_NP-complete_problems" target="_blank">can be found on Wikipedia</a>.
</p>

<div class="remark">
    One common misconception about \(\mathsf{NP}\)-complete problems is that they must have the most complex solutions imaginable: "if \(L \subseteq A^*\) is an \(\mathsf{NP}\)-complete decision problem that can be verified in \(\mathcal O(n^{5})\)-time, does that mean that there are no decision problems in \(\mathsf{NP}\) that require slower than \(\mathcal O(n^5)\)-time solutions?"
    The answer to this question is, of course, no.
    Completeness speaks of there being a polynomial-time reduction, and does not mention the runtime of that reduction: perhaps every reduction \(\mathcal S_y \colon U \preceq L\) from a given language \(U \subseteq A^*\) runs in \(\mathcal O(n^{17})\)-time. 
    Then the algorithm you obtain by composing the reduction with your verifier for \(L\) will run in \(\mathcal O(n^{17} + n^5) = \mathcal O(n^{17})\)-time as well.
</div>

<p>
    The significance of \(\mathsf{NP}\)-complete problems extends past their ubiquity in the gaming world. 
    Outside of the context of theory, many practical problems that we would love to be solved in some efficient manner are woefully \(\mathsf{NP}\)-complete.
    And what's worse, if anybody can come up with a polynomial time solution to any one of the many known \(\mathsf{NP}\)-complete problems, then that person has discovered that \(\mathsf{P} = \mathsf{NP}\) (an equation that is widely believed to be false).
</p>

<div class="theorem">
    <b>(The \(\mathsf{NP}\) to \(\mathsf{P}\) Pipeline)</b>
    Let \(L \subseteq A^*\) be a language, and assume that \(L\) is \(\mathsf{NP}\)-complete.
    If \(L \in \mathsf{P}\), then \(\mathsf{P} = \mathsf{NP}\).
</div>

<div class="proof">
    We already know that \(\mathsf{P} \subseteq \mathsf{NP}\).
    To see the opposite inclusion, let \(U \in \mathsf{NP}\) be any language verifiable in polynomial time. 
    Since \(L\) is \(\mathsf{NP}\)-hard, there is a polynomial time reduction \(U \preceq L\).
    By assumption, \(L \in \mathsf{P}\), so there is a polynomial time reduction from \(U\) to a tractable problem. 
    Thus, \(U \in \mathsf{P}\) as well.
</div>

<h2>Hard/Complete Problems</h2>

<p>
    We are now going to consider a few hard/complete problems.
    The first, and most important of these problems is the <i>satisfiability</i> problem.
    Most other \(\mathsf{NP}\)-complete problems are shown to be so by reducing the satisfiability problem to them.
</p>

<h3>Satisfiability</h3>

<p>
    In discrete math, you will have seen a bit of propositional logic.
    The basic ingredients were a set \(\mathbb P\) of <i>basic propositions</i>, and some <i>connectives</i> \(\wedge\) (and), \(\vee\) (or), and \(\neg\) (not) with which you could form new propositions (called <i>formulas</i>) from old ones. 
    More precisely, we can generate the set of formulas with a grammar.
</p>

<div class="definition">
    <b>(Propositional Formulas, Satisfiability)</b>
    Let \(\mathbb P\) be any set, elements of which we will call <i>basic propositions</i>.
    The set \(\mathit{Form}\) of all <i>propositional formulas</i> is generated by the grammar 
    \[
        F \to p \mid (F \wedge F) \mid (F \vee F) \mid (\neg F)
    \]
    where \(p \in \mathbb P\).

    <p></p>
    A <i>truth assignment</i> is a function \(\alpha \colon \mathbb P \to \{0,1\}\), indicating which basic propositions are to be taken as "false" or "true".
    The truth assignment \(\alpha\) extends to all propositional formulas via the truth table below: 
    \[\begin{array}{| c |c | c | c | c | c|}
        \hline
        \alpha(\varphi_1) & \alpha(\varphi_2) & \alpha(\varphi_1 \wedge \varphi_2) & \alpha(\varphi_1 \vee \varphi_2) & \alpha(\neg \varphi_1) \\
        \hline
        1 & 1 & 1 & 1 & 0 \\
        0 & 1 & 0 & 1 & 1 \\
        1 & 0 & 0 & 1 & 0 \\
        0 & 0 & 0 & 0 & 1 \\
        \hline
    \end{array}\]
    For a given formula \(\varphi\) and truth assignment \(\alpha \colon \mathbb P \to \{0,1\}\), we say that \(\alpha\) <i>satisfies</i> \(\varphi\) if \(\alpha(\varphi) = 1\).
    A formula \(\varphi\) is called <i>satisfiable</i> if there exists a truth assignment that satisfies it.
</div>

<p>
    As usual, we won't be writing all of the brackets into our propositional formulas.
    The order of operations for the connectives is:
    \[
        (\ ), ~ \neg, ~\wedge, ~\vee
    \]
    so for eg., \(\neg p \wedge q \vee r = (((\neg p) \wedge q) \vee r)\).
</p>

<div class="exercise">
    <b>(Satisfying Formula)</b>
    Determine which of the following formulas is satisfiable.
    <ol>
        <li>\(\neg p \wedge q \wedge \neg q\)</li>
        <li>\((p \vee q) \wedge (\neg q)\)</li>
        <li>\((p \wedge q) \vee (\neg q \wedge q) \vee (p \wedge \neg q)\)</li>
    </ol>
</div>

<p>
    The <i>satisfiability</i> problem is a decision problem involving propositional formulas in a certain format. 
</p>

<div class="definition">
    <b>(Conjunctive Normal Form)</b>
    A formula \(\varphi\) is a <i>literal</i> if \(\varphi = p\) or \(\varphi = \neg p\) for some \(p \in \mathbb P\).
    A <i>disjunction of literals</i> is a formula of the form 
    \[
        \varphi_1 \vee \varphi_2 \vee \cdots \vee \varphi_n
    \]
    where every \(\varphi_i\) is a literal.
    A formula is <i>in conjunctive normal form</i> (or <i>CNF</i>) if it is of the form 
    \[
        \varphi_1 \wedge \varphi_2 \wedge \cdots \wedge \varphi_n
    \]
    where this time, every \(\varphi_i\) is a disjunction of literals.
</div>

<p>
    Using DeMorgan's laws, one can show (by induction) that every propositional formula is logically equivalent to a formula in CNF.
    For a simple example, 
    \[\begin{aligned}
        \neg(p \wedge \neg q) \wedge \neg (q \vee r)
        = (\neg p \vee q) \wedge \neg q \wedge \neg r
    \end{aligned}\]
    The following problem, which Cook showed to be \(\mathsf{NP}\)-complete in 1971, deals with the complexity of determining whether a given propositional formula in CNF is satisfiable.
</p>

<div class="definition">
    <b>(Satisfiability)</b>
    The <i>satisfiability problem</i> is given by the language 
    \[
        \mathit{SAT} = \{\varphi \in \mathit{Form} \mid \text{\(\varphi\) is satisfiable and in CNF}\}
    \]
</div>

<p>
    It can be determined in linear time whether a given formula is in CNF (the string only has to be scanned from left to right once), so the real difficulty of the problem lies in finding a truth assignment that satifies the formula. 
    Note that given a formula in CNF, \(\varphi\), and given a truth assigment \(\alpha \colon \mathbb P \to \{0,1\}\), it can be determined in polynomial time whether \(\alpha\) satisfies \(\varphi\).
    Note that \(\varphi\) is by definition a string of symbols, so no string representation is needed here.
    Briefly, here is such a procedure: 
    <ol>
        <li>Scan \(\varphi\) from left to right and replace each instance of \(p\) with \(\alpha(p)\). Now rewind the tape and move on to step 2.</li>
        <li>
            Now scan from left to right and carry out the following replacements: 
            \[\begin{aligned}
                (1 \wedge 1) &\to 1 &
                (0 \wedge 1) &\to 0 \\
                (1 \wedge 0) &\to 0 &
                (0 \wedge 0) &\to 0 \\
                %
                (1 \vee 1) &\to 1 &
                (0 \vee 1) &\to 1 \\
                (1 \vee 0) &\to 1 &
                (0 \vee 0) &\to 0 \\
                %
                (\neg 1) &\to 0 &
                (\neg 0) &\to 1 
            \end{aligned}\]
        </li>
        <li>If the string on the tape consists of either \(0\) or \(1\), then halt. Else, go to 2.</li>
    </ol>
    The length of the string decreases with every repetition of step 2, so this algorithm runs in \(\mathcal O(n^2)\)-time. 
    Observe, then, that the nondeterministic Turing machine that runs the above procedure on all possible truth assignments (for the basic propositions appearing in \(\varphi\)) runs in nondeterministic \(\mathcal O(n^2)\)-time.
</p>

<div class="lemma">
    <b>(SAT is NP)</b>
    The satisfiability problem is verifiable in polynomial time, i.e., \(\mathit{SAT} \in \mathsf{NP}\).
</div>

<div class="exercise">
    <b>(Satisfying Formulas Algorithmically)</b>
    Run the satisfaction checking algorithm above on the input string \((((\neg p) \vee q) \wedge ((\neg q) \wedge (\neg r)))\) and the truth assignment \(\alpha(p) = 1\), \(\alpha(q) = 0\), and \(\alpha(r) = 1\).
</div>

<p>
    Interestingly, all other problems in \(\mathsf{NP}\) can efficiently be reduced to \(\mathit{SAT}\).
</p>

<div class="theorem">
    <b>(Cook-Levin, 1971)</b>
    The satisfiability problem \(\mathit{SAT}\) is \(\mathsf{NP}\)-complete.
</div>

<p>
    The original proof can be found on page 2 of this paper: <a href="https://dl.acm.org/doi/pdf/10.1145/800157.805047" target="_blank">(Cook, 1971)</a>.
    Wikipedia has a <a href="https://en.wikipedia.org/wiki/Cook%E2%80%93Levin_theorem" target="_blank">great rendition of the proof</a>, reproducing an argument found in section 2.6 of <a href="https://archive.org/details/computersintract0000gare/page/n15/mode/2up" target="_blank">Computers and Intractability, Gary and Johnson</a>.
    The gist of the proof is that, given any general Turing machine that verifies a given language \(L\) in nondeterministic polynomial time, one can form disjunctions of literals that describe different aspects of the runtime of the Turing machine, and then satisfiability of the conjunction of those literals determines whether the Turing machine has an accepting run (the halting behaviour of the Turing machine can be deduced from the satisfiability of the conjunction of these formulas).
    All of this leads to a very important technique for proving that a given decision problem is \(\mathsf{NP}\)-hard.
</p>

<div class="theorem">
    <b>(Reduction to SAT)</b>
    Let \(L \subseteq A^*\) be any language (decision problem).
    If there is a polynomial time reduction \(\mathit{SAT} \preceq L\), then \(L\) is \(\mathsf{NP}\)-hard.
</div>

<p>
    In the same paper, Cook proves something even more remarkable: a much simpler version of \(\mathit{SAT}\) is also \(\mathsf{NP}\)-complete.
    This one we are actually going to prove by hand.
</p>

<div class="definition">
    <b>(3-Bounded Conjunctive Normal Form)</b>
    A <i>3-disjunct of literals</i> is a formula of the form 
    \[
        \varphi_1 \vee \varphi_2 \vee \varphi_3
    \]
    where every \(\varphi_i\) is a literal.
    A formula is <i>in 3-conjunctive normal form</i> (or <i>3-CNF</i>) if it is of the form 
    \[
        \varphi_1 \wedge \varphi_2 \wedge \cdots \wedge \varphi_n
    \]
    where this time, every \(\varphi_i\) is a 3-disjunct of literals.
</div>

<div class="theorem">
    <b>(3SAT is NP-complete)</b>
    The <i>3-conjunct satisfiability problem</i> is the decision problem 
    \[
        \mathit{3SAT} = \{\varphi \in \mathit{Form} \mid \text{\(\varphi\) is satisfiable and in 3-CNF}\}
    \]
</div>

<div class="proof">
    First of all, \(\mathit{3SAT}\) is in \(\mathsf{NP}\) for the same reason that \(\mathit{SAT}\) is (the same algorithm works).
    <!-- Let us start by showing that \(\mathit{3SAT}\) is \(\mathsf{NP}\)-hard. -->
    For hardness, we are going to construct a polynomial time reduction \(r \colon \mathit{SAT} \preceq \mathit{3SAT}\).
    Then the Reduction to SAT theorem tells us that \(\mathit{3SAT}\) is \(\mathsf{NP}\)-hard.

    <p>
        The construction is rather simple, and runs in \(\mathcal O(n^2)\)-time.
        Recall that we need to show that every formula in CNF is equivalent to one in 3-CNF.
        We start with the following example to illustrate the process.
        
    </p>
</div>

<p>
    And of course, we obtain the corresponding technique for showing that a decision problem is \(\mathsf{NP}\)-hard.
</p>

<div class="theorem">
    <b>(Reduction to 3SAT)</b>
    Let \(L \subseteq A^*\) be any language (decision problem).
    If there is a polynomial time reduction \(\mathit{3SAT} \preceq L\), then \(L\) is \(\mathsf{NP}\)-hard.
</div>

<p>
    Many, many problems known to be \(\mathsf{NP}\)-hard/complete are proven to be so by reduction from \(\mathit{3SAT}\), because it is so simple to work with.
</p>

<!--  -->
<!-- <h3>The Travelling Salesman Problem</h3>

<p>
    A <i>weighted directed graph</i> is a pair \(\mathcal G = (X, \to)\) consisting of a set \(X\) of \emph{nodes} and a relation \({\to} \subseteq X \times \mathbb N \times X\) of \emph{weighted edges}.
    Given nodes \(x,y \in X\), the <i>cost of travel along a path</i> 
    \[
        P = (x \xrightarrow{c_1} x_1 \xrightarrow{c_2} \cdots \xrightarrow{c_n} x_n = y)
    \] 
    is the sum \(\mathsf{cost}(P) = \sum_{i = 1}^n c_i\).
    A <i>tour</i> of a (weighted) directed graph is a path 
    \[
        T = (x_0 \xrightarrow{c_1} x_1 \xrightarrow{c_2} x_2 \xrightarrow{c_3} \cdots \xrightarrow{c_n})
    \]
    such that \(X = \{x_0, x_1, \dots, x_n\}\).
</p>

<div class="definition">
    <b>(Travelling Salesman Decision Problem)</b>
    The <i>travelling salesman decision problem</i> is the decision problem 
    \[
        \mathit{TSDP} = \{(\mathcal G, k) \mid \text{there exists a tour \(T\) of \(\mathcal G\) with \(\mathsf{cost}(T) \le k\)}\}
    \]
</div>

<p>
    The travelling salesman decision problem is known to be \(\mathsf{NP}\)-complete.
</p>

<div class="theorem">
    <b>(TSDP is NP-complete)</b>
    The travelling salesman decision problem is \(\mathsf{NP}\)-complete.
</div>

<p>
    The proof of hardness is somewhat beyond us for now, but I'm sure you can figure out the completeness.
</p>

<div class="exercise">
    <b>(TSDP is NP)</b>
    Show that the travelling salesman decision problem is in \(\mathsf{NP}\).
</div> -->
